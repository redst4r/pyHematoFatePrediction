{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from talk_hemato_utils import load_data\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "from talk_hemato_utils import ismember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "To make it more accessible, I took the data provided with the [original code](http://github.com/QSCD/HematoFatePrediction), and rearange the data somewhat.\n",
    "Get the original data from [here](https://hmgubox.helmholtz-muenchen.de:8001/d/ccbfb5f1ac/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add inverted generations\n",
    "Let's put the information about inverted generation into the latent dataset directly, currently its stored in a separate hdf5 file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '../data/images_round3_test_latent_inverted_generations.pickle'  # the merged dataset will end up here\n",
    "latent_file = '../data_original/images_round3_test_latent.pickle'\n",
    "annotation_file = '../data_original/anno_file.h5'\n",
    "\n",
    "X_l, y_l, movement_l, cellIDs_l = load_data(latent_file, N=None, randomize=True)\n",
    "\n",
    "mat = h5py.File(annotation_file, 'r')\n",
    "gens = mat['anno']['latent'][:][0]\n",
    "gens = np.array([gens[i] for i in cellIDs_l])  # expand into a vector, one element for each patch\n",
    "\n",
    "assert len(gens) == len(X_l)== len(y_l)== len(movement_l)== len(cellIDs_l)\n",
    "\n",
    "with open(outfile, 'wb') as fh:\n",
    "    pickle.dump([X_l, y_l, movement_l, cellIDs_l, gens], fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrink the dataset sizes\n",
    "The original dataset (which is still only one experiment) is quite large (4GB+4GB).\n",
    "Let's shrink it to more managable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_ix(cellids, Ncells):\n",
    "    \"\"\"\n",
    "    selects a specd. number of cells.\n",
    "    cellids: a vector of cellids. \n",
    "             contains one element per image patch/sample, i.e. a single cellid will occur multiple times\n",
    "    Ncells: the desired number of cells to select at random\n",
    "    \n",
    "    returns: a boolean vector of cellids.shape; can be used to subindex the total dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_cells = np.unique(cellids)\n",
    "\n",
    "    cell_subset = np.random.choice(unique_cells, Ncells, replace=False)\n",
    "    subset_ix = ismember(cellids, cell_subset)\n",
    "    subset_ix = np.array([_ is not None for _ in subset_ix])\n",
    "\n",
    "    return subset_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated data\n",
    "We use only the testset in the notebooks anyways. Still the testset is quite large (in MB), so pick out a few cells (remember that one cell has multiple image patches). \n",
    "**Note**: I retrained the CNN, so we have to use that particular datasplit, and create a smaller version of that split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 251524 train data\n",
      " 83842 val. data\n",
      " 111789 test data\n"
     ]
    }
   ],
   "source": [
    "full_annotated_split = '../data/retrained_datasplit.pickle'\n",
    "outfile_annotated = '../data_small/small_retrained_datasplit.pickle'\n",
    "with open(full_annotated_split, 'rb') as fh:\n",
    "    X_train,X_val,X_test,\\\n",
    "    y_train,y_val,y_test,\\\n",
    "    mov_train, mov_val, mov_test,\\\n",
    "    cell_train, cell_val, cell_test = pickle.load(fh)\n",
    "\n",
    "print(\" %d train data\\n %d val. data\\n %d test data\" % (len(X_train), len(X_val), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11647 patches, 100 cells in total\n"
     ]
    }
   ],
   "source": [
    "Ncells = 100 \n",
    "subset_ix = create_subset_ix(cell_test, Ncells) # extract patches of 100 cells\n",
    "\n",
    "X_test_subset = X_test[subset_ix]\n",
    "y_test_subset = y_test[subset_ix]\n",
    "mov_test_subset = mov_test[subset_ix]\n",
    "cell_test_subset = cell_test[subset_ix]\n",
    "\n",
    "print(\"%d patches, %d cells in total\" % (len(X_test_subset), len(np.unique(cell_test_subset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outfile_annotated, 'wb') as fh:\n",
    "    pickle.dump([X_test_subset, y_test_subset, mov_test_subset, cell_test_subset], fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = '../data/images_round3_test_latent_inverted_generations.pickle'  # latent dataset + inverted generations\n",
    "with open(outfile, 'rb') as fh:\n",
    "    X_l, y_l, movement_l, cellIDs_l, gens = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ncells = 100\n",
    "subset_ix = create_subset_ix(cellIDs_l, Ncells) # extract patches of 100 cells\n",
    "\n",
    "X_l_subset = X_l[subset_ix]\n",
    "y_l_subset = y_l[subset_ix]\n",
    "mov_l_subset = movement_l[subset_ix]\n",
    "cell_l_subset = cellIDs_l[subset_ix]\n",
    "gens_l_subset = gens[subset_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some stats about that dataset. In particular, lets check how many cells in each inverted generation we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46800 patches, 100 cells in total\n",
      "\n",
      "patches per inv.gen\n",
      "{-5.0: 2353, inf: 20237, -1.0: 10017, -3.0: 5851, -2.0: 6031, -4.0: 2311}\n",
      "\n",
      "cells per inv.gen\n",
      "{-8: 0, -7: 0, -6: 0, -5: 4, -4: 6, -3: 13, -2: 15, -1: 21}\n"
     ]
    }
   ],
   "source": [
    "print(\"%d patches, %d cells in total\" % (len(X_l_subset), len(np.unique(cell_l_subset))))\n",
    "\n",
    "import toolz\n",
    "print('\\npatches per inv.gen')\n",
    "print(toolz.frequencies(gens_l_subset))\n",
    "\n",
    "print('\\ncells per inv.gen')\n",
    "print({_:np.unique(cell_l_subset[gens_l_subset==_]).shape[0] for _ in range(-8,0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_small/small_images_round3_test_latent_inverted_generations.pickle', 'wb') as fh:\n",
    "    pickle.dump([X_l_subset, y_l_subset, mov_l_subset, cell_l_subset, gens_l_subset], fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": "4",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
