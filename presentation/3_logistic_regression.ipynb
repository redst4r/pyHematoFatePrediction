{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic regression\n",
    "\n",
    "The journey to deep neural networks starts with logistic regression\n",
    "- it's the simplest neural network possible\n",
    "- training algorithm is pretty much the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from talk_utils import create_lin_data, create_nonlin_data\n",
    "from talk_plottingutils import *\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General supervised learning\n",
    "\n",
    "- dataset $(x_i, y_i), i=1\\ldots N$\n",
    "- $x_i \\in R^p$ are samples\n",
    "- $y_i$ are discrete labels\n",
    "\n",
    "build some predictor $\\hat y_i = f_\\theta(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy dataset\n",
    "A simple dataset, which is almost linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = create_lin_data(500)  # X,y = create_nonlin_data(500)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, alpha=0.5, cmap=plt.cm.bwr);\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "- most simple linear classifier\n",
    "    - $p_i = \\sigma(W x_i + b$)\n",
    "    - $y_i \\sim Bernoulli(p_i)$\n",
    "\n",
    "\n",
    "- **linear** decision boundary in input space\n",
    "![logreg](images/MLP/logreg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logreg(x, W, b):\n",
    "    return sigmoid(x@W + b)\n",
    "\n",
    "plt.plot(np.linspace(-5,5,30), sigmoid(np.linspace(-3,3,30)));\n",
    "plt.xlabel('x'); plt.ylabel('$\\sigma(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _grid_logreg(W,b):\n",
    "    x_range = np.linspace(-4,4,50)\n",
    "    y_range = np.linspace(-4,4,50)\n",
    "    P = np.zeros((50,50))\n",
    "    for i,ii in enumerate(y_range):\n",
    "        for j,jj in enumerate(x_range):\n",
    "            P[i,j] = logreg(np.array([ii,jj]).reshape(1,2), W,b)\n",
    "    return x_range, y_range, P\n",
    "\n",
    "def plot_2d_logreg(W,b):\n",
    "    x_range, y_range, P = _grid_logreg(W,b)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.pcolor(x_range, y_range, P, cmap=plt.cm.coolwarm)\n",
    "    plt.clim(0,1)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters of the logistic regression:\n",
    "W = np.array([[-0.5, 1]]).T\n",
    "b = np.array(-1).reshape(1,-1)\n",
    "\n",
    "plot_2d_logreg(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model\n",
    "aka fitting the model to the data\n",
    "\n",
    "### loss function\n",
    "- minimize a loss function $L(\\theta)$\n",
    "- likelihood of the data under the logistic regression model (just a Bernoulli): \n",
    "$$like(X,y| \\theta) = \\prod_{i=1\\ldots N} p_i^{y_i} \\cdot (1-p_i)^{1-y_i}$$\n",
    "- log likelihood: $$loglike(X,y| \\theta) = \\sum_{i=1\\ldots N} y_i \\log(p_i) +  (1-y_i) \\log(1-p_i)$$\n",
    "- turns out that this is (up to sign) the same as the *cross entropy loss*\n",
    "\n",
    "    \n",
    "### gradient descend:\n",
    "- minimization via **gradient descend**:\n",
    "- $\\theta_{t+1} := \\theta_{t} + \\eta \\cdot \\partial_\\theta L(\\theta)$\n",
    "- for our logistic regression example:\n",
    "    - $\\partial_W L(W,b) = \\sum_{i=1\\ldots N} y_i \\cdot (1-p_i) \\cdot \\partial_W (W x_i+b) + (1-y_i) \\cdot p_i \\cdot (-1) \\cdot\\partial_W (W x_i+b)$\n",
    "    - $\\partial_W L(W,b) = \\sum_{i=1\\ldots N} y_i \\cdot (1-p_i) \\cdot x_i - (1-y_i) \\cdot p_i \\cdot x_i$\n",
    "    - $\\partial_W L(W,b) = \\sum_{i=1\\ldots N} (y_i \\cdot (1-p_i) - (1-y_i) \\cdot p_i)\\cdot x_i$\n",
    "    - using $\\partial_x \\sigma(x) = \\sigma(x) \\cdot (1- \\sigma(x))$\n",
    "    \n",
    "    - analogous: $\\partial_b L(W,b) = \\sum_{i=1\\ldots N} y_i \\cdot (1-p_i) - (1-y_i) \\cdot p_i$\n",
    "    \n",
    "**Note**: For logistic regression, there's better way to fit than gradient descend (IRLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_gradW(x,y,W,b):\n",
    "    \"gradient wrt to W\"\n",
    "    p = logreg(x, W, b)\n",
    "    \n",
    "    if True: # slow but more explicit variant\n",
    "        gradients=[]\n",
    "        for i in range(len(y)): # for each datapoint calculate the gradient term\n",
    "            gradients.append(y[i]*(1-p[i])*x[i] - (1-y[i])*p[i] *x[i])\n",
    "        gradients = np.stack(gradients).sum(0,keepdims=True ).T\n",
    "    else:\n",
    "        gradients = (y.reshape(-1,1)*(1-p)*x - (1-y.reshape(-1,1))*p*x).sum(0,keepdims=True).T\n",
    "    \n",
    "    gradients = - gradients # sign flip due to likelihood vs loss\n",
    "    return gradients\n",
    "\n",
    "def logreg_gradb(x,y,W,b):\n",
    "    \"gradient wrt to b\"\n",
    "    p = logreg(x, W, b)\n",
    "    \n",
    "    if False:\n",
    "        gradients=[]\n",
    "        for i in range(len(y)):\n",
    "            gradients.append(y[i]*(1-p[i]) - (1-y[i])*p[i])\n",
    "        gradients = np.stack(gradients).sum(0,keepdims=True ).T\n",
    "    else:\n",
    "        gradients = (y.reshape(-1,1)*(1-p) - (1-y.reshape(-1,1))*p).sum(0,keepdims=True).T\n",
    "    \n",
    "    gradients = - gradients # sign flip due to likelihood vs loss\n",
    "    return gradients\n",
    "\n",
    "def logreg_loss(x,y,W,b):\n",
    "    \"the cross entropy loss\"\n",
    "    p = logreg(x, W, b)\n",
    "    loss = - np.sum(y * np.log(p) + (1-y) * np.log(1-p))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descend\n",
    "**Task 1**: Do a couple of gradient descend steps and plot the parameter evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W0 = np.array([[5],[-2]])  # initial condition\n",
    "b0 = np.array([-9])         # initial condition\n",
    "eta = 0.001                # learning rate\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/logreg-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the final decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_2d_logreg(W_list[-1],b_list[-1])\n",
    "plt.scatter(X[:,0], X[:,1], c=y, alpha=0.5, cmap=plt.cm.bwr, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the decision boundary while learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pgrid_list = [_grid_logreg(ww,bb) for ww,bb in zip(W_list, b_list)]\n",
    "hv_hmap = hv_plot_stack(Pgrid_list, range(len(Pgrid_list)))\n",
    "hv_scatter1= hv.Points(X[y==0])\n",
    "hv_scatter2= hv.Points(X[y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%output size=160\n",
    "%%opts Image style(cmap='bwr_r')\n",
    "%%opts Points (marker='o' size=10 alpha=0.5 line_color='k')\n",
    "hv_hmap * hv_scatter2 * hv_scatter1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": "4",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
