{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, InputLayer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from talk_plottingutils import plot_3d\n",
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A multilayer perceptron is a simple extension of logistic regression:\n",
    "## logistic regression: \n",
    "- $p_i = \\sigma(W x_i + b$)\n",
    "- $y_i \\sim Bernoulli(p_i)$\n",
    "\n",
    "- mapping the input directly to output: \n",
    "- **linear** decision boundary\n",
    "![logreg](images/MLP/logreg.png)\n",
    "\n",
    "## Multilayer perceptron:\n",
    "- we add another layer in between input and output\n",
    "- this **hidden layer** allows us to learn a different **representation of the input data**\n",
    "- the data might be linearly separable in the hidden layer, even though it wasnt in the input space\n",
    "![mlp](images/MLP/MLP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A new dataset\n",
    "not linearly separable, so logistic regression will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from talk_utils import create_nonlin_data\n",
    "X, y = create_nonlin_data(1000)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, alpha=0.2, cmap=plt.cm.bwr); plt.xlabel('x1'), plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Instead of implementing the MLP from scratch (calculating all the gradients etc), let's use [keras](https://keras.io), a high level library for NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP_factory():\n",
    "    MLP = Sequential()\n",
    "\n",
    "    #input layer\n",
    "    MLP.add(InputLayer(input_shape=(2,), name='input_layer'))\n",
    "\n",
    "    # hidden layer with 3 neurons/units\n",
    "    MLP.add(Dense(units=3, name='hidden_layer', activation='sigmoid'))\n",
    "\n",
    "    # output layer, outputs two class probabilities\n",
    "    MLP.add(Dense(units=2, name='output_layer', activation='softmax'))\n",
    "\n",
    "    return MLP\n",
    "\n",
    "MLP = MLP_factory() \n",
    "MLP.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit it, we must 'compile' the model and define the type of **optimization algorithm** to minimize the loss.\n",
    "<img src=\"images/MLP/gradient_descent.png\" alt=\"Watershedding\" style=\"width:400px;\" title=\"http://dsdeepdive.blogspot.com/2016/03/optimizations-of-gradient-descent.html\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a loss function and optimizer\n",
    "MLP.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',     # or 'sge' as simplest case\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_history = MLP.fit(X,to_categorical(y), epochs=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "subplot(121);plt.plot(training_history.epoch, training_history.history['loss']); \n",
    "plt.xlabel('Epoch');plt.ylabel('Loss');\n",
    "\n",
    "subplot(122);plt.plot(training_history.epoch, training_history.history['acc']); \n",
    "plt.xlabel('Epoch');plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Visualize the decision boundary (Hint: `MLP.predict()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/mlp-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The latent representation\n",
    "<img src=\"images/MLP/MLP.png\" alt=\"Watershedding\" style=\"width:300px;\">\n",
    "\n",
    "As discussed, the key is the latent representation, i.e. the hidden layer of the MLP.\n",
    "Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the slightly complicated way to get access to the activations of intermediate layers\n",
    "Hmodel = Model(inputs=MLP.get_layer('input_layer').input, \n",
    "               outputs=MLP.get_layer('hidden_layer').output)\n",
    "h = Hmodel.predict(X)  # that is the latent 3D representation of our data\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(h, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the MLP **learned a representation** where all red datapoints are pushed into one corner of the cube.\n",
    "- in this 3D representation, the two classes are **linearly separable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The latent representation during training\n",
    "How does this latent representation change during training?\n",
    "Here's a slightly hacky way to do a step-by-step gradient descend with keras recording the hidden layer (better use *callbacks*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MLP = MLP_factory() \n",
    "MLP.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.005), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h_vector = []\n",
    "for epoch in range(40):\n",
    "    # get latent rep\n",
    "    Hmodel = Model(inputs=MLP.get_layer('input_layer').input, \n",
    "                   outputs=MLP.get_layer('hidden_layer').output)\n",
    "    h = Hmodel.predict(X)  # that is the latent 3D representation of our data\n",
    "    h_vector.append(h)\n",
    "    \n",
    "    # update with one gradient step\n",
    "    MLP.fit(X,to_categorical(y), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternatively: step by step GD in tensorflow\n",
    "but using keras to define the layers. In tf, one would have to first implement the layer operations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "## model definitition\n",
    "img = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "H = Dense(3, activation='sigmoid', name='hidden_layer')(img)\n",
    "preds = Dense(2, activation='sigmoid', name='output_layer')(H)  # thats the predicted class scores\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 2))  # here , we feed in the true labels\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))  # just comparing prediction and truth\n",
    "\n",
    "# define a single step of gradient descend on the loss\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "h_vector = []\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        h_tmp = sess.run(H,feed_dict={img: X})\n",
    "        h_vector.append(h_tmp)\n",
    "        train_step.run(feed_dict={img: X,\n",
    "                                  labels: to_categorical(y,2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation\n",
    "using holoviews to display the latent respresentation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('matplotlib')\n",
    "scatter_dict = {i: hv.Scatter3D(h[y==0][:200])*hv.Scatter3D(h[y==1][:200]) for i,h in enumerate(h_vector[:200])} \n",
    "hmap = hv.HoloMap(scatter_dict, kdims=['Epoch'])\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Neural Net\n",
    "\n",
    "- just add more layer between input and output\n",
    "- more nonlinearities\n",
    "\n",
    "![DNN](images/MLP/DNN.png)\n",
    "\n",
    "## GoogLenet\n",
    "![DNN](images/MLP/googlenet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": "4",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
